# -*- coding: utf-8 -*-
"""Quotesnstories.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18rdXOsgoVmsc51Rcj9L3ebxazEwZdg91
"""

#Running and buliding the recommender system on given dummy data

#Import the required libraries
import numpy as np
import pandas as pd

#Making dataframe with the given dataset
df = pd.DataFrame()

id =  [0,1,2,3]
website_name = ['Gitlab.com','Cloudbees.com','squarespace.com','Wix.com']
Service = ['From project planning and source code management to CI/CD and monitoring, GitLab is a complete DevOps platform, delivered as a single application. Only GitLab enables Concurrent DevOps to make the software lifecycle 200% faster','Reduce risk, optimize software delivery and accelerate innovation with CloudBees - the industry-leading DevOps technology platform. Build Stuff That Matters','Squarespace is the all-in-one solution for anyone looking to create a beautiful website. Domains, eCommerce, hosting, galleries, analytics, and 24/7 support all included','Create a free website with Wix.com. Choose a stunning template and customize anything with the Wix website builder—no coding skills needed. Create yours today!']

df['id'] = id
df['website'] = website_name
df['Service'] = Service

df

#Data cleaning steps
#Parsing the service column
from html.parser import HTMLParser
html_parser = HTMLParser()
df['Service'] = df['Service'].apply(lambda x: html_parser.unescape(x))
df

# Appostophes = {"'s":"is","'re":"are"}
# words = df['Service']).split()
# df['Service'] = [Appostophes[word] if word in Appostophes else word for word in words]
# df['Service'] = "".join(df['Service'])

#Lowering all the words
import re
df['Service']= df['Service'].apply(lambda x: x.lower())

#Extracting only words from Service
df['Service'] = df['Service'].apply(lambda x: re.sub(r'#[\W]*',' ',x))

df

df["Service"] = df["Service"].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))
df['Service']

#More data clearning
Appostophes = {"'s":"is","'re":"are"}
df['Service']
for i in range(len(df)):
  words =  df['Service'][i]
  for word in words:
    if word == "'s":
      word= "is"
    elif word == "'re":
      word =  "are"
  df['Service'][i] = words

df['Service']

documents = list(df['Service'])
print(documents)
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

#Extracting entities from text
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
def extract_entities(name, text):
    # Lopping over the sentences of the text
    for sent in nltk.sent_tokenize(text):
        # nltk.word_tokeize returns a list of words composing a sentence
        # nltk.pos_tag returns the position tag of words in the sentence
        # nltk.ne_chunk returns a label to each word based on this position tag when possible
        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):
            try:
                if chunk.label() == 'PERSON':
                    for c in chunk.leaves():
                        if str(c[0].lower()) not in name:
                            name.append(str(c[0]).lower())
            except AttributeError:
                pass
    return name
## 
names = []
for doc in documents:
    names = extract_entities(names, doc)
## Update the stop words list
stop_words.update(names)

#Stemming the similar words

from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer("english") # Choose a language
texts = [[stemmer.stem(word) for word in document.lower().split() if (word not in stop_words)]
          for document in documents]

print(texts)

# Now the main part comes using gensim it will only create matrices based on unique words and if related words then it will treat as same both the words
from gensim import corpora, models, similarities
dictionary = corpora.Dictionary(texts)

#Showing  the created dictionary
import itertools
list(itertools.islice(dictionary.token2id.items(), 0, 20))

#creating corpus from the words in the dictionary
corpus = [dictionary.doc2bow(text) for text in texts]
corpus

#Here the first element in dict is the position of the  tag in the dictionary

print(corpus[3][:20])

dictionary[48]

#creating the tfidf for the created corpus 
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
corpus_tfidf

print(corpus_tfidf[3][:20])

#Converting the gensim matrix into sparse matrix to run the cosine similarities on them

import gensim
import numpy as np

corpus_tfidf_csr = gensim.matutils.corpus2csc(corpus_tfidf)
corpus_tfidf_numpy = corpus_tfidf_csr.T.toarray()

#Running the cosine similarities

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel 
cosine_similarities = linear_kernel(corpus_tfidf_numpy, corpus_tfidf_numpy) 
results = {}
idx = 0
row = 0
count = 0
for idx, row in df.iterrows():
   similar_indices = cosine_similarities[idx].argsort()[:-100:-1] 
   similar_items = [(cosine_similarities[idx][i], df['id'][i]) for i in similar_indices] 
   results[row['id']] = similar_items[1:]

#Checking the results

def item(id):  
  return df.loc[df['id'] == id]['website'].tolist()[0].split(' - ')[0] 

def recommend(item_id, num):
    print("Recommending " + str(num) + " products similar to " + item(item_id) + "...")   
    print("-------")    
    recs = results[item_id][:num]   
    for rec in recs: 
       print("Recommended: " + item(rec[1]) + " (score:" +      str(rec[0]) + ")")

#Recommendation results but the performance is low 

#Lets run the created model on bigger dataset
recommend(item_id=1, num=2)

#Now running the model on g2 data

import numpy as np
import pandas as pd
df = pd.DataFrame()

id =  [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
website = ['Cloudflare','Amazon CloudFront','Fastly','CDN77','KeyCDN','Imperva Incapsula','Linodes Standard','Google Cloud CDN','Bootstrapcdn','StackPath CDN','Rackspace CDN','Microsoft Azure CDN','Akamai','Sucuri','Kollective','CDNify','Cachefly','Dexecure','IBM Enterprise Video Streaming','CDNetworks']

Service = ['What We Do At Cloudflare, we have our eyes set on an ambitious goal -- to help build a better Internet. We believe that with our talented team, smart technology and engaged users we can solve some of the biggest problems on the Internet','Amazon CloudFront is a web service for content delivery. It integrates with other Amazon Web Services to give developers and businesses an easy way to distribute content to end users with low latency, high data transfer speeds, and no commitments','Fastly helps the world’s most popular digital businesses keep pace with their customer expectations by delivering fast, secure, and scalable online experiences','CDN77 is a Content Delivery Network with 35+ points of presence around the world (incl. 3 in South America). CDN77 has its own Video Processing and Delivery platform called Streamflow','KeyCDN is a simple, fast & reliable Content Delivery Network with 25+ POPs strategicially located around the globe. KeyCDN offers the lowest price at $0.04/GB (every region), pay-as-you-go and many advanced features such as HTTP/2 Support, Free SSL, Instant Purging, a RESTful API, Real-time Analytics, Two-Factor Auth, and 24+ CMS integrations (WordPress, Drupal, Joomla, Magento, etc',"Imperva Incapsula delivers an enterprise-grade Web Application Firewall to safeguard your site from the latest threats, an intelligent and instantly effective 360-degree anti-DDoS solutions (layers 3-4 and 7), a global CDN to speed up your website's load speed and minimize bandwidth usage and an array of performance monitoring and analytic services to provide insights about your website's security and performance","Linode accelerates innovation by making cloud computing simple, accessible, and affordable to all. Founded in 2003, Linode helped pioneer the cloud computing industry and is today the largest independent open cloud provider in the world. Headquartered in Philadelphia's Old City, the company empowers more than a million developers, startups, and businesses across its global network of data centers",'Content Delivery Network for Cloud Platform Google Cloud CDN leverages Google\' s globally distributed edge points of presence to accelerate content delivery for websites and applications served out of Google Compute Engine and Google Cloud Storage. Cloud CDN lowers network latency, offloads origins, and reduces serving costs. Once you\'ve set up HTTP(S) Load Balancing, simply enable Cloud CDN with a single checkbox','Bootstrapcdn is the recommended CDN for Bootstrap, Font Awesome and Bootswatch','Innovative Content Delivery Network providing cutting-edge web services and info. like EdgeSSL and DualCDN Strategies','Accelerate your websites, web applications, and media delivery','A fast and modern global delivery network for high-bandwidth content','The Akamai Intelligent Platform is a cloud platform for delivering secure, high-performing user experiences to any device, anywhere. It reaches globally and delivers locally','Sucuri is a managed security service provider for websites. Our cloud-based tools provide complete website security solution, including performance optimization via a CDN, mitigation of external attacks like vulnerability exploits and DDoS attacks, and professional response in the event of security incident','At Kollective, we make corporate networks smarter so your people can work better. The Kollective platform scales existing IT infrastructure to accelerate content delivery to the edge of the enterprise while minimizing network congestion','CDNify is a flexible and easy-to-use content delivery network built for developers, startups, and digital agencies to accelerate your website, speed up load times and provide a faster, more reliable experience for your customers','Launched in 2002, CacheFly developed the world’s first TCP-anycast based Content Delivery Network—the only network built for throughput. From the first byte to the last byte, CacheFly delivers your files faster','We are passionate web performance experts that want to make the internet faster. Dexecure is the only automatic web performance solution that accelerates websites, for better business results. Dexecure automatically performs Front end optimizations and partners with Content Delivery Networks (CDN) to deliver the fastest loading site','IBM Watson Media (formerly IBM Cloud Video) provides an enterprise video solution, powered by Watson AI, for creating, storing, managing, broadcasting and measuring the impact of live and recorded video','CDNetworks enables Global Cloud Acceleration with 160 global points of presence including China, Russia and other emerging markets']

df['id'] = id
df['website'] = website
df['Service'] = Service

df

#Data cleaning steps
from html.parser import HTMLParser
html_parser = HTMLParser()
df['Service'] = df['Service'].apply(lambda x: html_parser.unescape(x))
df

import re
df['Service']= df['Service'].apply(lambda x: x.lower())

#Extracting only words from Service
df['Service'] = df['Service'].apply(lambda x: re.sub(r'#[\W]*',' ',x))

df

df["Service"] = df["Service"].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))
df['Service']

Appostophes = {"'s":"is","'re":"are"}
df['Service']
for i in range(len(df)):
  words =  df['Service'][i]
  for word in words:
    if word == "'s":
      word= "is"
    elif word == "'re":
      word =  "are"
  df['Service'][i] = words

df['Service']

documents = list(df['Service'])
print(documents)
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
def extract_entities(name, text):
    # Lopping over the sentences of the text
    for sent in nltk.sent_tokenize(text):
        # nltk.word_tokeize returns a list of words composing a sentence
        # nltk.pos_tag returns the position tag of words in the sentence
        # nltk.ne_chunk returns a label to each word based on this position tag when possible
        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):
            try:
                if chunk.label() == 'PERSON':
                    for c in chunk.leaves():
                        if str(c[0].lower()) not in name:
                            name.append(str(c[0]).lower())
            except AttributeError:
                pass
    return name
## 
names = []
for doc in documents:
    names = extract_entities(names, doc)
## Update the stop words list
stop_words.update(names)

from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer("english")
texts = [[stemmer.stem(word) for word in document.lower().split() if (word not in stop_words)]
          for document in documents]

print(texts)

from gensim import corpora, models, similarities
dictionary = corpora.Dictionary(texts)

import itertools
list(itertools.islice(dictionary.token2id.items(), 0, 20))

corpus = [dictionary.doc2bow(text) for text in texts]
corpus

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

corpus_tfidf

import gensim
import numpy as np

corpus_tfidf_csr = gensim.matutils.corpus2csc(corpus_tfidf)
corpus_tfidf_numpy = corpus_tfidf_csr.T.toarray()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel 
cosine_similarities = linear_kernel(corpus_tfidf_numpy, corpus_tfidf_numpy) 
results = {}
idx = 0
row = 0
count = 0
for idx, row in df.iterrows():
   similar_indices = cosine_similarities[idx].argsort()[:-100:-1] 
   similar_items = [(cosine_similarities[idx][i], df['id'][i]) for i in similar_indices] 
   results[row['id']] = similar_items[1:]

#Checking the results

def item(id):  
  return df.loc[df['id'] == id]['website'].tolist()[0].split(' - ')[0] 

def recommend(item_id, num):
    print("Recommending " + str(num) + " products similar to " + item(item_id) + "...")   
    print("-------")    
    recs = results[item_id][:num]   
    for rec in recs: 
       print("Recommended: " + item(rec[1]) + " (score:" +      str(rec[0]) + ")")

#Recommendations results again the performance is not that much good but still it works
recommend(item_id=1,num=5)